{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1808590,"sourceType":"datasetVersion","datasetId":989445}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Sentement Analysis Using Machine Learning**\n\n> ### **Dataset Description**\n\nThe sentiment analysis dataset is generated automatically, using emoticons as indicators of sentiment polarity: positive emoticons (:) interpreted as positive tweets, while negative emoticons (:() indicated negative tweets. The dataset includes six fields structured as CSV files:\n\nPolarity represents the tweet's sentiment polarity (0 for negative, 2 for neutral, and 4 for positive),\nTweet ID represents a unique identifier for each tweet,\nTweet Date represents the date and time of the tweet in UTC format,\nQuery specifies the query term used in the tweet, or 'NO_QUERY' if there is no query,\nThe username of the Twitter user who posted the tweet,\nText indicates the tweet's actual content, without emoticons.\nThis dataset is designed to be used by the global data science community for research on sentiment analysis experiments. This allows researchers to explore techniques and algorithms for analysing emotions.","metadata":{}},{"cell_type":"markdown","source":"### Import Libraries & DataSet","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport string\nfrom string import punctuation \nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import LancasterStemmer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.metrics import confusion_matrix\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', 255)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Loading the Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/sentiment-analysis-dataset/train.csv\", encoding='latin1')\ntest = pd.read_csv(\"/kaggle/input/sentiment-analysis-dataset/test.csv\", encoding='latin1')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Concatenate Training and Testing Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.concat([df,test])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Checking the INFO of the Dataset","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dropping the Unnecessory Columns","metadata":{}},{"cell_type":"code","source":"df.drop(columns=['textID','Time of Tweet', 'Age of User', 'Country', 'Population -2020', 'Land Area (Km²)', 'Density (P/Km²)', \"selected_text\"], axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I have Dropped all the unnecessory Columns","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Checking the unique values of sentiment column\ndf['sentiment'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.dropna(inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Basic Preprocessing**\n\n- Remove tags - HTML\n- Lower case\n- remove stopwords","metadata":{}},{"cell_type":"code","source":"df.head(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **remove_tags**","metadata":{}},{"cell_type":"code","source":"def remove_tags(raw_text):\n    cleaned_text = re.sub(re.compile('<.*?>'), '', str(raw_text))\n    return cleaned_text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['cleaned_text'] = df['text'].apply(remove_tags)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Making all the Text in Lower case","metadata":{}},{"cell_type":"code","source":"df['cleaned_text'] = df['text'].apply(lambda x:str(x).lower())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def remove_unnecessary_characters(text):\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', str(text))\n    text = re.sub(r'\\s+', ' ', str(text)).strip()\n    return text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['cleaned_text'] = df['text'].apply(remove_unnecessary_characters)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Normalizing the Dataset","metadata":{}},{"cell_type":"code","source":"def normalize_text(text):\n    if isinstance(text, str):\n        text = re.sub(r'[^\\w\\s]', '', text)\n        text = re.sub(r'\\s+', ' ', text).strip()\n    else:\n        text = str(text)\n    return text\ndf['cleaned_text'] = df['text'].apply(normalize_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Removing Stepwords From Dataset","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nimport nltk\n\nnltk.download('stopwords')\nsw_list = stopwords.words('english')\n\ndf['cleaned_text'] = df['text'].apply(lambda x: [item for item in x.split() if item not in sw_list]).apply(lambda x:\" \".join(x))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.tail(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Removing URLS","metadata":{}},{"cell_type":"code","source":"def remove_urls(text):\n    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    pattern = re.compile(r'http?://\\S+|www\\.\\S+')\n    return pattern.sub(r'', text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['cleaned_text'] = df['text'].apply(remove_urls)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"exclude = string.punctuation\nexclude\n\ndef remove_punc1(text):\n    return text.translate(str.maketrans('', '', exclude))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['cleaned_text'] = df['text'].apply(remove_punc1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Splitting the Dataset","metadata":{}},{"cell_type":"code","source":"X = df['cleaned_text']\ny = df['sentiment']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Encoding the Categorical Values","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\n\ny = encoder.fit_transform(y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Applying BoW","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nX_train_bow = cv.fit_transform(X_train).toarray()\nX_test_bow = cv.transform(X_test).toarray()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### USing GaussianNB","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\n\ngnb.fit(X_train_bow,y_train)     ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = gnb.predict(X_test_bow)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score,classification_report, ConfusionMatrixDisplay\nscore_lr = accuracy_score(y_test, y_pred)\nscore_lr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Using Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(n_jobs=-1)\nlr.fit(X_train_bow,y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = lr.predict(X_test_bow)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score,classification_report, ConfusionMatrixDisplay\nscore_lr = accuracy_score(y_test, y_pred)\nscore_lr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(random_state=0)\nrfc.fit(X_train_bow, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_rfc = rfc.predict(X_test_bow)\nscore_rfc = rfc.score(X_test_bow, y_test)\nscore_rfc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ConfusionMatrixDisplay.from_predictions(y_test, y_pred);","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def wp(text):\n    return text.upper()\n\ndef output_lable(n):\n    if n == 0:\n        return \"The Text Sentement is Negative\"\n    elif n == 1:\n        return \"The Text Sentement is Neutral\"\n    elif n == 2:\n        return \"The Text Sentement is Positive\"\n    \ndef manual_testing(news):\n    testing_news = {\"text\":[news]}\n    new_def_test = pd.DataFrame(testing_news)\n    new_def_test[\"text\"] = new_def_test[\"text\"].apply(wp) \n    new_x_test = new_def_test[\"text\"]\n    new_xv_test = cv.transform(new_x_test)\n    pred_lr = lr.predict(new_xv_test)\n    pred_rfc = rfc.predict(new_xv_test)\n\n    return print((output_lable(pred_lr)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text = \"I am very Happy \"\nmanual_testing(text)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}